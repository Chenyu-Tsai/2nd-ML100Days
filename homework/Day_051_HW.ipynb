{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import date\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss, roc_auc_score, auc, roc_curve\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "DATA_ROOT = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1160742, 7)\n",
      "(306313, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_id</th>\n",
       "      <th>Merchant_id</th>\n",
       "      <th>Coupon_id</th>\n",
       "      <th>Discount_rate</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Date_received</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1439408</td>\n",
       "      <td>2632</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20160217.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1439408</td>\n",
       "      <td>2632</td>\n",
       "      <td>8591.0</td>\n",
       "      <td>20:1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20160217.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1439408</td>\n",
       "      <td>2632</td>\n",
       "      <td>1078.0</td>\n",
       "      <td>20:1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20160319.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1832624</td>\n",
       "      <td>3381</td>\n",
       "      <td>7610.0</td>\n",
       "      <td>200:20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20160429.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2029232</td>\n",
       "      <td>3381</td>\n",
       "      <td>11951.0</td>\n",
       "      <td>200:20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160129.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_id  Merchant_id  Coupon_id Discount_rate  Distance  Date_received  \\\n",
       "0  1439408         2632        NaN           NaN       0.0            NaN   \n",
       "1  1439408         2632     8591.0          20:1       0.0     20160217.0   \n",
       "2  1439408         2632     1078.0          20:1       0.0     20160319.0   \n",
       "3  1832624         3381     7610.0        200:20       0.0     20160429.0   \n",
       "4  2029232         3381    11951.0        200:20       1.0     20160129.0   \n",
       "\n",
       "         Date  \n",
       "0  20160217.0  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfoff = pd.read_csv(os.path.join(DATA_ROOT,'train_offline.csv'))\n",
    "#dfoff = dfoff.iloc[:5000]\n",
    "dftest = pd.read_csv(os.path.join(DATA_ROOT,'test_offline.csv'))\n",
    "#dftest = dftest.iloc[:5000]\n",
    "dftest = dftest[~dftest.Coupon_id.isna()]\n",
    "dftest.reset_index(drop=True, inplace=True)\n",
    "print(dfoff.shape)\n",
    "print(dftest.shape)\n",
    "dfoff.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "DistanceFilling_UM = dfoff.groupby(['User_id','Merchant_id'])['Distance'].mean().reset_index()\n",
    "DistanceFilling_UM.columns = ['User_id','Merchant_id','DistanceFilling_UM']\n",
    "DistanceFilling_U = dfoff.groupby(['User_id'])['Distance'].mean().reset_index()\n",
    "DistanceFilling_U.columns = ['User_id','DistanceFilling_U']\n",
    "DistanceFilling_M = dfoff.groupby(['Merchant_id'])['Distance'].mean().reset_index()\n",
    "DistanceFilling_M.columns = ['Merchant_id','DistanceFilling_M']\n",
    "\n",
    "DistanceFilling_UM = dftest.groupby(['User_id','Merchant_id'])['Distance'].mean().reset_index()\n",
    "DistanceFilling_UM.columns = ['User_id','Merchant_id','DistanceFilling_UM']\n",
    "DistanceFilling_U = dftest.groupby(['User_id'])['Distance'].mean().reset_index()\n",
    "DistanceFilling_U.columns = ['User_id','DistanceFilling_U']\n",
    "DistanceFilling_M = dftest.groupby(['Merchant_id'])['Distance'].mean().reset_index()\n",
    "DistanceFilling_M.columns = ['Merchant_id','DistanceFilling_M']\n",
    "\n",
    "dfoff = pd.merge(dfoff,DistanceFilling_UM,on = ['User_id','Merchant_id'], how = 'left')\n",
    "dfoff = pd.merge(dfoff,DistanceFilling_U,on = ['User_id'], how = 'left')\n",
    "dfoff = pd.merge(dfoff,DistanceFilling_M,on = ['Merchant_id'], how = 'left')\n",
    "\n",
    "\n",
    "dftest = pd.merge(dftest,DistanceFilling_UM,on = ['User_id','Merchant_id'], how = 'left')\n",
    "dftest = pd.merge(dftest,DistanceFilling_U,on = ['User_id'], how = 'left')\n",
    "dftest = pd.merge(dftest,DistanceFilling_M,on = ['Merchant_id'], how = 'left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DistanceMissingFill(data):\n",
    "    if np.isnan(data['Distance']):\n",
    "        if not np.isnan(data['DistanceFilling_UM']):\n",
    "            return int(data['DistanceFilling_UM'])\n",
    "        elif not np.isnan(data['DistanceFilling_U']):\n",
    "            return int(data['DistanceFilling_U'])\n",
    "        elif not  np.isnan(data['DistanceFilling_M']):\n",
    "            return int(data['DistanceFilling_M'])\n",
    "    return data['Distance']\n",
    "\n",
    "dfoff['Distance'] = dfoff.apply(DistanceMissingFill, axis = 1)\n",
    "dftest['Distance'] = dftest.apply(DistanceMissingFill, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1124438\n",
       "1      36304\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creat target label \n",
    "\"\"\"\n",
    "According to the definition, \n",
    "1) buy with coupon within (include) 15 days ==> 1\n",
    "2) buy with coupon but out of 15 days ==> 0\n",
    "3) buy without coupon ==> -1 (we don't care)\n",
    "\"\"\"\n",
    "def label(row):\n",
    "    if np.isnan(row['Date_received']):\n",
    "        return 0\n",
    "    if not np.isnan(row['Date']):\n",
    "        td = pd.to_datetime(row['Date'], format='%Y%m%d') -  pd.to_datetime(row['Date_received'], format='%Y%m%d')\n",
    "        if td <= pd.Timedelta(15, 'D'):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "dfoff[\"label\"] = dfoff.apply(label, axis=1)\n",
    "dfoff[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWeekday(row):\n",
    "    if (np.isnan(row)) or (row==-1):\n",
    "        return row\n",
    "    else:\n",
    "        return pd.to_datetime(row, format = \"%Y%m%d\").dayofweek+1 # add one to make it from 0~6 -> 1~7\n",
    "\n",
    "dfoff['weekday'] = dfoff['Date_received'].apply(getWeekday)\n",
    "dftest['weekday'] = dftest['Date_received'].apply(getWeekday)\n",
    "# weekday_type (weekend = 1)\n",
    "dfoff['weekday_type'] = dfoff['weekday'].apply(lambda x : 1 if x in [6,7] else 0 ) # apply to trainset\n",
    "dftest['weekday_type'] = dftest['weekday'].apply(lambda x : 1 if x in [6,7] else 0 ) # apply to testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確認消費時間是否為 4 個假期\n",
    "def check_new_year(x):\n",
    "    start = datetime.date(2016, 2, 3)\n",
    "    end = datetime.date(2016, 2, 13)\n",
    "    if (np.isnan(x)) or (x==-1):\n",
    "        return 0\n",
    "    else:\n",
    "        if start <= pd.to_datetime(x, format = \"%Y%m%d\") <= end:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "def check_child(x):\n",
    "    start = datetime.date(2016, 4, 2)\n",
    "    end = datetime.date(2016, 4, 5)\n",
    "    if (np.isnan(x)) or (x==-1):\n",
    "        return 0\n",
    "    else:\n",
    "        if start <= pd.to_datetime(x, format = \"%Y%m%d\") <= end:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "def check_mother(x):\n",
    "    start = datetime.date(2016, 5, 1)\n",
    "    end = datetime.date(2016, 5, 8)\n",
    "    if (np.isnan(x)) or (x==-1):\n",
    "        return 0\n",
    "    else:\n",
    "        if start <= pd.to_datetime(x, format = \"%Y%m%d\") <= end:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0       \n",
    "\n",
    "dfoff['new_year'] = dfoff['Date_received'].apply(check_new_year)\n",
    "dfoff['child'] = dfoff['Date_received'].apply(check_child)\n",
    "dfoff['mother'] = dfoff['Date_received'].apply(check_mother)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest['new_year'] = dftest['Date_received'].apply(check_new_year)\n",
    "dftest['child'] = dftest['Date_received'].apply(check_child)\n",
    "dftest['mother'] = dftest['Date_received'].apply(check_mother)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate features - coupon discount and distance\n",
    "def getDiscountType(row):\n",
    "    if np.isnan(float(row.replace(':',''))):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def convertRate(row):\n",
    "    \"\"\"Convert discount to rate\"\"\"\n",
    "    if np.isnan(float(row.replace(':',''))):\n",
    "        return 1\n",
    "    elif ':' in row:\n",
    "        rows = row.split(':')\n",
    "        return 1.0 - float(rows[1])/float(rows[0])\n",
    "    else:\n",
    "        return float(row)\n",
    "\n",
    "def getDiscountJian(row):\n",
    "    if ':' in row:\n",
    "        rows = row.split(':')\n",
    "        return int(rows[1])\n",
    "    else:\n",
    "        return 0\n",
    "def getDiscountMan(row):\n",
    "    if ':' in row:\n",
    "        rows = row.split(':')\n",
    "        return int(rows[0])\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def processData(df):\n",
    "    \n",
    "    # convert discunt_rate\n",
    "    df['discount_rate'] = df['Discount_rate'].astype('str').apply(convertRate)\n",
    "    df['discount_man'] = df['Discount_rate'].astype('str').apply(getDiscountMan)\n",
    "    df['discount_jian'] = df['Discount_rate'].astype('str').apply(getDiscountJian)\n",
    "    df['discount_type'] = df['Discount_rate'].astype('str').apply(getDiscountType)\n",
    "    \n",
    "    # convert distance\n",
    "    df.loc[df.Distance.isna(), \"Distance\"] = 99\n",
    "    \n",
    "            \n",
    "            \n",
    "    \n",
    "    return df\n",
    "\n",
    "dfoff = processData(dfoff)\n",
    "dftest = processData(dftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_feature = ['discount_rate',\n",
    "                    'discount_type',\n",
    "                    'discount_jian',\n",
    "                    'discount_man',\n",
    "                    'Distance', \n",
    "                    'weekday_type',\n",
    "                    'new_year',\n",
    "                    'dragon',\n",
    "                    'mother',\n",
    "                    'child',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_X = dfoff[cnn_feature].values\n",
    "training_Y = dfoff['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 580371 samples, validate on 580371 samples\n",
      "Epoch 1/100\n",
      "580371/580371 [==============================] - 6s 11us/step - loss: 0.1369 - categorical_accuracy: 0.9680 - val_loss: 0.1072 - val_categorical_accuracy: 0.9687\n",
      "Epoch 2/100\n",
      "580371/580371 [==============================] - 5s 9us/step - loss: 0.1090 - categorical_accuracy: 0.9688 - val_loss: 0.1066 - val_categorical_accuracy: 0.9687\n",
      "Epoch 3/100\n",
      "580371/580371 [==============================] - 5s 8us/step - loss: 0.1072 - categorical_accuracy: 0.9688 - val_loss: 0.1067 - val_categorical_accuracy: 0.9687\n",
      "Epoch 4/100\n",
      "580371/580371 [==============================] - 4s 7us/step - loss: 0.1066 - categorical_accuracy: 0.9688 - val_loss: 0.1052 - val_categorical_accuracy: 0.9687\n",
      "Epoch 5/100\n",
      "580371/580371 [==============================] - 5s 8us/step - loss: 0.1061 - categorical_accuracy: 0.9688 - val_loss: 0.1050 - val_categorical_accuracy: 0.9687\n",
      "Epoch 6/100\n",
      "580371/580371 [==============================] - 5s 9us/step - loss: 0.1058 - categorical_accuracy: 0.9688 - val_loss: 0.1049 - val_categorical_accuracy: 0.9687\n",
      "Epoch 7/100\n",
      "580371/580371 [==============================] - 5s 9us/step - loss: 0.1055 - categorical_accuracy: 0.9688 - val_loss: 0.1049 - val_categorical_accuracy: 0.9687\n",
      "Epoch 8/100\n",
      "204800/580371 [=========>....................] - ETA: 2s - loss: 0.1056 - categorical_accuracy: 0.9689"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-188-da79c0435760>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m           callbacks=[es])\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras import metrics\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=10, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = 'adam',\n",
    "              metrics = [metrics.categorical_accuracy])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "model.fit(training_X, to_categorical(training_Y),\n",
    "          epochs=100,\n",
    "          batch_size=1024,\n",
    "          shuffle=True,\n",
    "          validation_split=0.5,\n",
    "          callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_X = dftest[cnn_feature].values\n",
    "results = model.predict(test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetset = dftest.copy()\n",
    "testset = targetset[cnn_feature].copy()\n",
    "test1 = testset.copy()\n",
    "\n",
    "test1['pred_prob'] = results[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(306313, 4)\n"
     ]
    }
   ],
   "source": [
    "output = pd.concat((targetset[[\"User_id\", \"Coupon_id\", \"Date_received\"]], test1[\"pred_prob\"]), axis=1)\n",
    "print(output.shape)\n",
    "\n",
    "output.loc[:, \"User_id\"] = output[\"User_id\"].apply(lambda x:str(int(x)))\n",
    "output.loc[:, \"Coupon_id\"] = output[\"Coupon_id\"].apply(lambda x:str(int(x)))\n",
    "output.loc[:, \"Date_received\"] = output[\"Date_received\"].apply(lambda x:str(int(x)))\n",
    "output[\"uid\"] = output[[\"User_id\", \"Coupon_id\", \"Date_received\"]].apply(lambda x: '_'.join(x.values), axis=1)\n",
    "output.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000020_2705_20160519</td>\n",
       "      <td>0.233459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000020_8192_20160513</td>\n",
       "      <td>0.233459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000065_1455_20160527</td>\n",
       "      <td>0.110296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000085_8067_20160513</td>\n",
       "      <td>0.140469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000086_2418_20160613</td>\n",
       "      <td>0.140469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     uid     label\n",
       "0  1000020_2705_20160519  0.233459\n",
       "1  1000020_8192_20160513  0.233459\n",
       "2  1000065_1455_20160527  0.110296\n",
       "3  1000085_8067_20160513  0.140469\n",
       "4  1000086_2418_20160613  0.140469"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = output.groupby(\"uid\", as_index=False).mean()\n",
    "out = out[[\"uid\", \"pred_prob\"]]\n",
    "out.columns = [\"uid\", \"label\"]\n",
    "out.to_csv(\"cnn_adam_2.csv\", header=[\"uid\", \"label\"], index=False) # submission format\n",
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tuneParamsRandom(classifier, params, train_x, train_y, cv = 5):\n",
    "    rs = RandomizedSearchCV(classifier, params, n_iter = 20, scoring = 'roc_auc', n_jobs = -1, verbose = 0, cv=cv)\n",
    "    rs.fit(train_x, train_y)\n",
    "    \n",
    "    return rs.best_params_, abs(rs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "etParams = {'n_estimators':np.arange(100,1100,50), 'max_depth':np.arange(3,11,2),\n",
    "            'min_samples_leaf':np.arange(1,6,2) }\n",
    "\n",
    "rfParams = {'n_estimators':np.arange(100,1100,50), 'max_depth':np.arange(3,11,2),\n",
    "            'min_samples_split':np.arange(2,20,2), 'min_samples_leaf':np.arange(1,6,2) }\n",
    "\n",
    "abParams = {'n_estimators':np.arange(100,1100,50),'learning_rate':np.arange(0.01,0.2,0.05)}\n",
    "\n",
    "\n",
    "lgbmcParams = {'n_estimators' : np.arange(400,1200,100), 'learning_rate' : np.arange(0.01,0.1,0.02),\n",
    "               'num_leaves' : np.arange(2,48,4), 'max_depth' : np.arange(3,10,2),\n",
    "               'subsample' : np.arange(0.3,0.8,0.1) }\n",
    "\n",
    "gbcParams = {'n_estimators' : np.arange(400,1200,100) , 'learning_rate' : np.arange(0.01,0.1,0.02) \n",
    "             , 'min_samples_split' : np.arange(2,30,5), 'min_samples_leaf' : np.arange(2,32,4),\n",
    "              'max_depth' : np.arange(3,8,2), 'subsample' : np.arange(0.3,0.8,0.1)}\n",
    "\n",
    "xgbParams = {'max_depth':np.arange(3,8,1),'learning_rate':np.arange(0.01,0.1,0.02)\n",
    "             ,'n_estimator': np.arange(1000,3000,100),'gamma':np.arange(0.01,0.1,0.02)}\n",
    "\n",
    "lrParams = {'C':np.arange(0.01,1,0.05), 'max_iter' : np.arange(100,500,100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(580371, 11)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label = dfoff.label\n",
    "train_x = dfoff[cnn_feature]\n",
    "\n",
    "train_label.index = train_x.index\n",
    "tuneSet = pd.concat([train_x,train_label], axis = 1)\n",
    "trainSet = tuneSet.sample(frac=0.5)\n",
    "trainSet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(406259, 10)\n",
      "(406259,)\n",
      "(174112, 10)\n",
      "(174112,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(trainSet[trainSet.columns[trainSet.columns != 'label']],trainSet['label'], test_size = 0.3, random_state = 1234)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier: {'subsample': 0.4, 'n_estimators': 500, 'min_samples_split': 17, 'min_samples_leaf': 18, 'max_depth': 5, 'learning_rate': 0.049999999999999996} 0.8886836355236886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Distance         0.250913\n",
       "discount_man     0.245383\n",
       "discount_rate    0.224453\n",
       "discount_type    0.122303\n",
       "discount_jian    0.119005\n",
       "weekday_type     0.017789\n",
       "new_year         0.015599\n",
       "child            0.004555\n",
       "mother           0.000000\n",
       "dragon           0.000000\n",
       "Name: importances, dtype: float64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(max_features = 'sqrt')\n",
    "gbc_best_Params,gbc_best_score = tuneParamsRandom(gbc,gbcParams,x_train,y_train)\n",
    "print(\"GradientBoostingClassifier:\",gbc_best_Params,gbc_best_score)\n",
    "\n",
    "\n",
    "gbc = GradientBoostingClassifier(**gbc_best_Params,max_features = 'sqrt')\n",
    "gbc.fit(train_x, train_label)\n",
    "gbc_pred = gbc.predict_proba(test_X)[:,1]\n",
    "\n",
    "importances = pd.DataFrame(gbc.feature_importances_, columns = ['importances'], index = train_x.columns)\n",
    "importances.importances.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMClassifier: {'subsample': 0.5, 'num_leaves': 22, 'n_estimators': 800, 'max_depth': 3, 'learning_rate': 0.08999999999999998} 0.8883883312041179\n"
     ]
    }
   ],
   "source": [
    "lgbmc_best_Params, lgbmc_best_score = tuneParamsRandom(LGBMClassifier(),lgbmcParams,x_train,y_train)\n",
    "print(\"LGBMClassifier:\",lgbmc_best_Params,lgbmc_best_score)\n",
    "\n",
    "\n",
    "lgbmc = LGBMClassifier(**lgbmc_best_Params)\n",
    "lgbmc.fit(train_x,train_label)\n",
    "lgbmc_pred = lgbmc.predict_proba(test_X)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier: {'n_estimator': 2700, 'max_depth': 6, 'learning_rate': 0.08999999999999998, 'gamma': 0.049999999999999996} 0.8885360141291645\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['discount_rate', 'discount_type', 'discount_jian', 'discount_man', 'Distance', 'weekday_type', 'new_year', 'dragon', 'mother', 'child'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9']\nexpected child, dragon, discount_type, new_year, discount_rate, weekday_type, Distance, mother, discount_man, discount_jian in input data\ntraining data did not have the following fields: f7, f2, f9, f5, f0, f1, f6, f8, f4, f3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-f83b7553d100>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mxgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mxgb_best_Params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mxgb_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, data, ntree_limit, validate_features)\u001b[0m\n\u001b[1;32m    832\u001b[0m         class_probs = self.get_booster().predict(test_dmatrix,\n\u001b[1;32m    833\u001b[0m                                                  \u001b[0mntree_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mntree_limit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m                                                  validate_features=validate_features)\n\u001b[0m\u001b[1;32m    835\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multi:softprob\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mclass_probs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, output_margin, ntree_limit, pred_leaf, pred_contribs, approx_contribs, pred_interactions, validate_features)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidate_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_bst_ulong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_validate_features\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m                 raise ValueError(msg.format(self.feature_names,\n\u001b[0;32m-> 1690\u001b[0;31m                                             data.feature_names))\n\u001b[0m\u001b[1;32m   1691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_split_value_histogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_pandas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: feature_names mismatch: ['discount_rate', 'discount_type', 'discount_jian', 'discount_man', 'Distance', 'weekday_type', 'new_year', 'dragon', 'mother', 'child'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9']\nexpected child, dragon, discount_type, new_year, discount_rate, weekday_type, Distance, mother, discount_man, discount_jian in input data\ntraining data did not have the following fields: f7, f2, f9, f5, f0, f1, f6, f8, f4, f3"
     ]
    }
   ],
   "source": [
    "xgb_best_Params,xgb_best_score = tuneParamsRandom(XGBClassifier(),xgbParams, x_train, y_train)\n",
    "print(\"XGBClassifier:\",xgb_best_Params,xgb_best_score)\n",
    "\n",
    "xgb = XGBClassifier(**xgb_best_Params)\n",
    "xgb.fit(train_x,train_label)\n",
    "xgb_pred = xgb.predict_proba(test_X)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = dftest[dfoff[cnn_feature].columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(**xgb_best_Params)\n",
    "xgb.fit(train_x,train_label)\n",
    "xgb_pred = xgb.predict_proba(test_X)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "blending_pred = gbc_pred * 0.34 + lgbmc_pred * 0.33 + xgb_pred * 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "gbc = GradientBoostingClassifier(**gbc_best_Params)\n",
    "lgbmc = LGBMClassifier(**lgbmc_best_Params)\n",
    "xgb = XGBClassifier(**xgb_best_Params)\n",
    "\n",
    "#gbc = GradientBoostingClassifier()\n",
    "#lgbmc = LGBMClassifier()\n",
    "#xgb = XGBClassifier()\n",
    "\n",
    "meta_estimator = LGBMClassifier()\n",
    "stacking = StackingClassifier(classifiers = [gbc,lgbmc, xgb], meta_classifier = meta_estimator)\n",
    "stacking.fit(x_train,y_train)\n",
    "stacking_pred = stacking.predict_proba(test_X)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1['pred_prob'] = blending_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(306313, 4)\n"
     ]
    }
   ],
   "source": [
    "output = pd.concat((targetset[[\"User_id\", \"Coupon_id\", \"Date_received\"]], test1[\"pred_prob\"]), axis=1)\n",
    "print(output.shape)\n",
    "\n",
    "output.loc[:, \"User_id\"] = output[\"User_id\"].apply(lambda x:str(int(x)))\n",
    "output.loc[:, \"Coupon_id\"] = output[\"Coupon_id\"].apply(lambda x:str(int(x)))\n",
    "output.loc[:, \"Date_received\"] = output[\"Date_received\"].apply(lambda x:str(int(x)))\n",
    "output[\"uid\"] = output[[\"User_id\", \"Coupon_id\", \"Date_received\"]].apply(lambda x: '_'.join(x.values), axis=1)\n",
    "output.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000020_2705_20160519</td>\n",
       "      <td>0.246100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000020_8192_20160513</td>\n",
       "      <td>0.246100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000065_1455_20160527</td>\n",
       "      <td>0.128948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000085_8067_20160513</td>\n",
       "      <td>0.142277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000086_2418_20160613</td>\n",
       "      <td>0.142277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     uid     label\n",
       "0  1000020_2705_20160519  0.246100\n",
       "1  1000020_8192_20160513  0.246100\n",
       "2  1000065_1455_20160527  0.128948\n",
       "3  1000085_8067_20160513  0.142277\n",
       "4  1000086_2418_20160613  0.142277"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = output.groupby(\"uid\", as_index=False).mean()\n",
    "out = out[[\"uid\", \"pred_prob\"]]\n",
    "out.columns = [\"uid\", \"label\"]\n",
    "out.to_csv(\"blending_pred.csv\", header=[\"uid\", \"label\"], index=False) # submission format\n",
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
